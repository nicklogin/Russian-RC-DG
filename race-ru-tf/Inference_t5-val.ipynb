{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bafc561-654b-4e23-a49f-c9fd1a6e43a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import evaluate\n",
    "import torch as tt\n",
    "import pandas as pd\n",
    "\n",
    "from typing import Any, Dict, Union\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tqdm import tqdm_notebook\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, PreTrainedModel, PreTrainedTokenizer\n",
    "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9a4a8fe-e042-4861-9b22-7f38059b03ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[nltk_data] Downloading package wordnet to /home/user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/user/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# models:\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"ai-forever/ruT5-base\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"RuT5-RACE-tf-1/checkpoint-65760\").to(tt.device(\"cuda:0\"))\n",
    "\n",
    "# metrics:\n",
    "bleu4 = evaluate.load(\"bleu\")\n",
    "sbleu = evaluate.load(\"sacrebleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "meteor = evaluate.load(\"meteor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e77ddc5d-fb1b-4859-a1b1-5b339acb3248",
   "metadata": {},
   "outputs": [],
   "source": [
    "option_id_dict = {\n",
    "    'A': 0, 'B': 1, 'C': 2, 'D': 3\n",
    "}\n",
    "\n",
    "def to_new_format(example: dict[str, Union[str, list[str]]]) -> str:\n",
    "  inp, label = '', ''\n",
    "  example[\"options_ru\"] = [option for option in example[\"options_ru\"] if option]\n",
    "  right_answer = example['options_ru'][option_id_dict[example['answer']]]\n",
    "\n",
    "  right_answer = right_answer.replace('\"', \"'\")\n",
    "\n",
    "  qtext_orig = example[\"question\"].lower()\n",
    "  if (\"not true\" in qtext_orig) or (\"false\" in qtext_orig) or (\"n't true\" in qtext_orig) or (\"untrue\" in qtext_orig):\n",
    "      if (\"not false\" in qtext_orig) or (\"n't false\" in qtext_orig):\n",
    "          inp += example['article_ru'] + \" \" + \"ВОПРОС: Какое высказывание СООТВЕТСТВУЕТ тексту? \"\n",
    "      else:\n",
    "          inp += example['article_ru'] + \" \" + \"ВОПРОС: Какое высказывание НЕ СООТВЕТСТВУЕТ тексту? \"\n",
    "  else:\n",
    "      inp += example['article_ru'] + \" \" + \"ВОПРОС: Какое высказывание СООТВЕТСТВУЕТ тексту? \"\n",
    "\n",
    "  inp += f'ПРАВИЛЬНЫЙ ОТВЕТ: \"{right_answer}\".'\n",
    "  inp += 'НЕПРАВИЛЬНЫЕ ВАРИАНТЫ ОТВЕТА: '\n",
    "\n",
    "  options = example[\"options_ru\"]\n",
    "  options = [\n",
    "      option.replace('\"', \"'\") for option in options if option != right_answer\n",
    "  ]\n",
    "  options = [\n",
    "      f'\"{option}\"' for option in options\n",
    "  ]\n",
    "  label = \"; \".join(options)\n",
    "\n",
    "  distractors_len = len(tokenizer(label)[\"input_ids\"])\n",
    "    \n",
    "  return {\"inp\": inp, \"right_answer\": right_answer, \"distractors\": label, \"distractors_len\": distractors_len}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b03d6cee-05ee-40a2-9860-f80bc20828e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bf1d280bb674d26824c4f0002d59da6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3288 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be8d5a75af78462baa7e7f65ba2a33ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/175 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eec9d42c58f247bea05ebce4dd4c0c27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/187 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"tf_dataset_pretty_filtered.json\", 'r', encoding=\"utf8\") as inp:\n",
    "    tf_dataset = json.load(inp)\n",
    "\n",
    "tf_dataset_train, tf_dataset_val, tf_dataset_test = tf_dataset[\"train\"], tf_dataset[\"val\"], tf_dataset[\"test\"]\n",
    "tf_dataset_train = Dataset.from_list(tf_dataset_train)\n",
    "tf_dataset_val = Dataset.from_list(tf_dataset_val)\n",
    "tf_dataset_test = Dataset.from_list(tf_dataset_test)\n",
    "\n",
    "tf_dataset_train = tf_dataset_train.map(to_new_format)\n",
    "tf_dataset_val = tf_dataset_val.map(to_new_format)\n",
    "tf_dataset_test = tf_dataset_test.map(to_new_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cdb0e2e-852a-4971-86cb-97a4629e19cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    3288.000000\n",
       "mean       47.546533\n",
       "std        11.146655\n",
       "min        21.000000\n",
       "25%        40.000000\n",
       "50%        46.000000\n",
       "75%        54.000000\n",
       "max       136.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(tf_dataset_train[\"distractors_len\"]).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de702f29-456c-454e-a17e-c2ce042f3b01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LEN = pd.Series(tf_dataset_train[\"distractors_len\"]).quantile(0.99)\n",
    "MAX_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a31a9d34-3a7d-4935-b715-490af7b63f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(text: str, model: PreTrainedModel) -> str:\n",
    "    input_ = tokenizer([text], return_tensors=\"pt\")\n",
    "    output = model.generate(\n",
    "        input_[\"input_ids\"].to(tt.device(\"cuda:0\"))\n",
    "    )\n",
    "    return tokenizer.batch_decode(output)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cab0fc91-1bcb-415b-8f68-42f4b83c02ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Почему люди скрещивают пальцы, сталкиваясь с трудностями нерелигиозных людей? Почему мы виним чёрного кота, когда видим низкий балл теста?\n",
      "Эти привычки называются волшебным мышлением и можно найти везде в повседневной жизни. Например, человек хочет чего - то плохого для раздражающего коллеги. Или родители молятся о безопасном бою, когда их сын уходит. \n",
      "Психологическая помощь\n",
      "Изучая эти привычки, ученые решили найти причину, по которой люди верят в магические силы. Психологи имеют свой собственный подход к этому вопросу. \n",
      "\"Я думаю, что отчасти это потому, что мы постоянно подвергаемся воздействию наших собственных мыслей, и поэтому, скорее всего, переоценим их связь с внешними событиями. \"Сказала Эмили Пронин, психолог в префиксе = st1 /US.\n",
      "Для людей, которые не уверены в своих способностях или медленных действиях, волшебное мышление может быть большой помощью, объяснил доктор Дэниел Вегнер, профессор психологии Гарварда. Чувство, что их собственные мысли могут контролировать вещи, может помочь людям бороться с депрессией. \n",
      "Предательство, приобретенное в результате эволюции\n",
      "Но у эволюционистов есть свои собственные идеи. \n",
      "Две школы эволюционного мышления придумали причины, анализируя развитие человека. Они являются « адаптационными » и « теоретиками побочных продуктов ». \n",
      "Адаптационисты говорят, что вера в магические силы заставляет людей чувствовать себя лучше, меньше беспокоиться о трудностях, больше фокусироваться на будущем и больше заботиться о себе. У людей с сильной верой (в чем-то) больше шансов выжить в трудной среде или соперничестве. \n",
      "Но в теории побочных продуктов магическое мышление является продуктом наших психологических особенностей. \n",
      "\"Мы автоматически ищем объяснение того, почему что-то происходит,\" объясняет Джастин Барретт, психолог, мозг, таким образом, эволюционировал, чтобы быстро судить о причинно-следственной связи. Так что мы часто связываем два события, основываясь не более чем на совпадении. Например, « Я просто думала о том, чтобы найти свою старую школьную подружку, когда она вдруг позвонила мне ».\n",
      "Еще одна психологическая особенность — это теория разума. Мы признаем, что может существовать невидимая сила ума, которая влияет на исход инцидента. \n",
      "Теоретики побочных продуктов утверждают, что из-за этих особенностей мы рождаемся с тенденцией верить в наше волшебное мышление. \n",
      "Но нам нужно ограничить наше волшебное мышление, предупредить психологов. \n",
      "Для большинства людей убеждения - это просто утешительный частный ритуал. Когда на карту поставлено что-то важное, например испытание, представление или отношения, люди не просто совершают свои личные ритуалы. Они должны подготовиться. ВОПРОС: Какое высказывание СООТВЕТСТВУЕТ тексту? ПРАВИЛЬНЫЙ ОТВЕТ: \"Доктор Дэниел Вегнер считает, что магическое мышление может помочь.\".НЕПРАВИЛЬНЫЕ ВАРИАНТЫ ОТВЕТА: \n"
     ]
    }
   ],
   "source": [
    "print(tf_dataset_test[42][\"inp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff58be34-a643-490d-96c4-f2770bbda765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Джастин Барретт, эволюционист, отрицает, что магическое мышление является продуктом адаптации.\"; \"У людей со слабой верой больше шансов выжить в соперничестве.\"; \"Адаптационисты думают, что мы рождаемся с тенденцией верить в наше волшебное мышление.\"\n"
     ]
    }
   ],
   "source": [
    "print(tf_dataset_test[42][\"distractors\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75f754c2-9ccd-4db8-985d-675aa698a5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad>\"Эволюция привела к теории разума.\"; \"Люди рождаются с тенденцией\n"
     ]
    }
   ],
   "source": [
    "predict_model = model_predict(tf_dataset_test[42][\"inp\"], model)\n",
    "print(predict_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65e6d37f-d061-44f0-bea1-9fdb351726b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "У тебя есть какие-нибудь замечательные планы на предстоящие зимние каникулы? Вот замечательные фильмы, чтобы убить время. ВОПРОС: Какое высказывание СООТВЕТСТВУЕТ тексту? ПРАВИЛЬНЫЙ ОТВЕТ: \"Сиротка - это комедия об Эстер, усыновленной доброй семьёй.\".НЕПРАВИЛЬНЫЕ ВАРИАНТЫ ОТВЕТА: \n"
     ]
    }
   ],
   "source": [
    "print(tf_dataset_test[100][\"inp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd2243a7-765e-4eb0-b878-02b99a9aee4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"500 дней лета - роман с счастливым концом.\"; \"Мы идем показывать красивые пейзажи во время путешествия молодых пар.\"; \"Древнее предсказание майя произойдет в 2012 году.\"\n"
     ]
    }
   ],
   "source": [
    "print(tf_dataset_test[100][\"distractors\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5de933df-0f05-4a50-8ad7-c05a6bc11c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad>\"Сротка - комедия об Эстер.\"; \"Сильмовая комедия об Э\n"
     ]
    }
   ],
   "source": [
    "predict_model = model_predict(tf_dataset_test[100][\"inp\"], model)\n",
    "print(predict_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "979b414b-cdeb-4d86-a363-9df4ab25ebdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad>\"Сротка - комедия об Эстер.\"; \"Сильмовая комедия об Э\n"
     ]
    }
   ],
   "source": [
    "predict_model = model_predict(tf_dataset_test[100][\"inp\"], model)\n",
    "print(predict_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7e0bcd7-c20e-4dd8-9518-b59c0650c72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method generate in module transformers.generation.utils:\n",
      "\n",
      "generate(inputs: Optional[torch.Tensor] = None, generation_config: Optional[transformers.generation.configuration_utils.GenerationConfig] = None, logits_processor: Optional[transformers.generation.logits_process.LogitsProcessorList] = None, stopping_criteria: Optional[transformers.generation.stopping_criteria.StoppingCriteriaList] = None, prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], List[int]]] = None, synced_gpus: Optional[bool] = None, assistant_model: Optional[ForwardRef('PreTrainedModel')] = None, streamer: Optional[ForwardRef('BaseStreamer')] = None, negative_prompt_ids: Optional[torch.Tensor] = None, negative_prompt_attention_mask: Optional[torch.Tensor] = None, **kwargs) -> Union[transformers.generation.utils.GenerateDecoderOnlyOutput, transformers.generation.utils.GenerateEncoderDecoderOutput, transformers.generation.utils.GenerateBeamDecoderOnlyOutput, transformers.generation.utils.GenerateBeamEncoderDecoderOutput, torch.LongTensor] method of transformers.models.t5.modeling_t5.T5ForConditionalGeneration instance\n",
      "    Generates sequences of token ids for models with a language modeling head.\n",
      "    \n",
      "    <Tip warning={true}>\n",
      "    \n",
      "    Most generation-controlling parameters are set in `generation_config` which, if not passed, will be set to the\n",
      "    model's default generation configuration. You can override any `generation_config` by passing the corresponding\n",
      "    parameters to generate(), e.g. `.generate(inputs, num_beams=4, do_sample=True)`.\n",
      "    \n",
      "    For an overview of generation strategies and code examples, check out the [following\n",
      "    guide](../generation_strategies).\n",
      "    \n",
      "    </Tip>\n",
      "    \n",
      "    Parameters:\n",
      "        inputs (`torch.Tensor` of varying shape depending on the modality, *optional*):\n",
      "            The sequence used as a prompt for the generation or as model inputs to the encoder. If `None` the\n",
      "            method initializes it with `bos_token_id` and a batch size of 1. For decoder-only models `inputs`\n",
      "            should be in the format of `input_ids`. For encoder-decoder models *inputs* can represent any of\n",
      "            `input_ids`, `input_values`, `input_features`, or `pixel_values`.\n",
      "        generation_config (`~generation.GenerationConfig`, *optional*):\n",
      "            The generation configuration to be used as base parametrization for the generation call. `**kwargs`\n",
      "            passed to generate matching the attributes of `generation_config` will override them. If\n",
      "            `generation_config` is not provided, the default will be used, which has the following loading\n",
      "            priority: 1) from the `generation_config.json` model file, if it exists; 2) from the model\n",
      "            configuration. Please note that unspecified parameters will inherit [`~generation.GenerationConfig`]'s\n",
      "            default values, whose documentation should be checked to parameterize generation.\n",
      "        logits_processor (`LogitsProcessorList`, *optional*):\n",
      "            Custom logits processors that complement the default logits processors built from arguments and\n",
      "            generation config. If a logit processor is passed that is already created with the arguments or a\n",
      "            generation config an error is thrown. This feature is intended for advanced users.\n",
      "        stopping_criteria (`StoppingCriteriaList`, *optional*):\n",
      "            Custom stopping criteria that complements the default stopping criteria built from arguments and a\n",
      "            generation config. If a stopping criteria is passed that is already created with the arguments or a\n",
      "            generation config an error is thrown. If your stopping criteria depends on the `scores` input, make\n",
      "            sure you pass `return_dict_in_generate=True, output_scores=True` to `generate`. This feature is\n",
      "            intended for advanced users.\n",
      "        prefix_allowed_tokens_fn (`Callable[[int, torch.Tensor], List[int]]`, *optional*):\n",
      "            If provided, this function constraints the beam search to allowed tokens only at each step. If not\n",
      "            provided no constraint is applied. This function takes 2 arguments: the batch ID `batch_id` and\n",
      "            `input_ids`. It has to return a list with the allowed tokens for the next generation step conditioned\n",
      "            on the batch ID `batch_id` and the previously generated tokens `inputs_ids`. This argument is useful\n",
      "            for constrained generation conditioned on the prefix, as described in [Autoregressive Entity\n",
      "            Retrieval](https://arxiv.org/abs/2010.00904).\n",
      "        synced_gpus (`bool`, *optional*):\n",
      "            Whether to continue running the while loop until max_length. Unless overridden this flag will be set to\n",
      "            `True` under DeepSpeed ZeRO Stage 3 multiple GPUs environment to avoid hanging if one GPU finished\n",
      "            generating before other GPUs. Otherwise it'll be set to `False`.\n",
      "        assistant_model (`PreTrainedModel`, *optional*):\n",
      "            An assistant model that can be used to accelerate generation. The assistant model must have the exact\n",
      "            same tokenizer. The acceleration is achieved when forecasting candidate tokens with the assistent model\n",
      "            is much faster than running generation with the model you're calling generate from. As such, the\n",
      "            assistant model should be much smaller.\n",
      "        streamer (`BaseStreamer`, *optional*):\n",
      "            Streamer object that will be used to stream the generated sequences. Generated tokens are passed\n",
      "            through `streamer.put(token_ids)` and the streamer is responsible for any further processing.\n",
      "        negative_prompt_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "            The negative prompt needed for some processors such as CFG. The batch size must match the input batch\n",
      "            size. This is an experimental feature, subject to breaking API changes in future versions.\n",
      "        negative_prompt_attention_mask (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "            Attention_mask for `negative_prompt_ids`.\n",
      "        kwargs (`Dict[str, Any]`, *optional*):\n",
      "            Ad hoc parametrization of `generation_config` and/or additional model-specific kwargs that will be\n",
      "            forwarded to the `forward` function of the model. If the model is an encoder-decoder model, encoder\n",
      "            specific kwargs should not be prefixed and decoder specific kwargs should be prefixed with *decoder_*.\n",
      "    \n",
      "    Return:\n",
      "        [`~utils.ModelOutput`] or `torch.LongTensor`: A [`~utils.ModelOutput`] (if `return_dict_in_generate=True`\n",
      "        or when `config.return_dict_in_generate=True`) or a `torch.LongTensor`.\n",
      "    \n",
      "            If the model is *not* an encoder-decoder model (`model.config.is_encoder_decoder=False`), the possible\n",
      "            [`~utils.ModelOutput`] types are:\n",
      "    \n",
      "                - [`~generation.GenerateDecoderOnlyOutput`],\n",
      "                - [`~generation.GenerateBeamDecoderOnlyOutput`]\n",
      "    \n",
      "            If the model is an encoder-decoder model (`model.config.is_encoder_decoder=True`), the possible\n",
      "            [`~utils.ModelOutput`] types are:\n",
      "    \n",
      "                - [`~generation.GenerateEncoderDecoderOutput`],\n",
      "                - [`~generation.GenerateBeamEncoderDecoderOutput`]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model.generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d178e98e-e2ea-4328-977f-636dedb14793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metric_inputs_seq2seq(\n",
    "    input_batch: list[str], #label_batch: list[str],\n",
    "    model: PreTrainedModel, tokenizer: PreTrainedTokenizer\n",
    ") -> list[str]:\n",
    "    input_batch_ = tokenizer(input_batch, return_tensors=\"pt\", padding=True)[\"input_ids\"].to(tt.device(\"cuda:0\"))\n",
    "    # label_batch_ = tokenizer(label_batch, return_tensors=\"pt\", padding=True)[\"input_ids\"]\n",
    "\n",
    "    # output_length = label_batch_.shape[-1]\n",
    "\n",
    "    with tt.no_grad():\n",
    "        output_batch = model.generate(input_batch_, max_length=MAX_LEN)\n",
    "\n",
    "    output = [\n",
    "        sent.replace(\"<pad>\", \" \").replace(\"</s>\", \" \").strip() for sent in tokenizer.batch_decode(output_batch) # \n",
    "    ]\n",
    "    \n",
    "    del input_batch_\n",
    "    del output_batch\n",
    "    # del label_batch_\n",
    "    tt.cuda.empty_cache()\n",
    "\n",
    "    return output\n",
    "\n",
    "def compute_metrics(output: list[str], label_batch: list[str]) -> dict:\n",
    "    metric_dict = {\n",
    "        \"bleu\": bleu4.compute(predictions=output, references=[[label] for label in label_batch]),\n",
    "        \"sbleu\": sbleu.compute(predictions=output, references=[[label] for label in label_batch]),\n",
    "        \"rouge\": rouge.compute(predictions=output, references=label_batch),\n",
    "        \"meteor\": meteor.compute(predictions=output, references=label_batch)\n",
    "    }\n",
    "    return metric_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae179de2-929f-4df7-adf6-d17318117596",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = tf_dataset_test[\"inp\"][:16]\n",
    "right_answers = tf_dataset_test[\"right_answer\"][:16]\n",
    "labels = [item.replace('\\n', '').replace('  ',' ').replace('  ',' ').strip() for item in tf_dataset_test[\"distractors\"][:16]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d727f17-ee18-4171-b071-aa76bc18c6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_model = get_metric_inputs_seq2seq(model_inputs, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd7c8153-42be-4533-ba00-6fab71357fcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"Самые великие земледелцы пустыни - это люди.\"; \"Пустыни быстро растут.\"; \"Размеры пустыни постоянно меняются.\"',\n",
       " '\"Нельсон Мандела не был его оригинальным именем.\"; \"Нельсон Мандела был назван своим учителем.\"; \"Нельсон Мандела основал свою собственную юридическую фирму до того, как получил степень юриста.\"',\n",
       " '\"Детям нехорошо веселиться летом.\"; \"Детям будет скучно читать программы\"; \"Учителям не нужно помогать детям анализировать уроки.\"',\n",
       " '\"Фестиваль Гластонбери работает на прибыльной основе.\"; \"Джеймс Браун и Джосс Стоун родились в бедных семьях.\"; \"В 1970 году на фестивале Гластонбери можно бесплатно пообедать на ферме.\"',\n",
       " '\"Университет находится в центре города.\"; \"Студенты могут жить на улице.\"; \"Университет не заинтересован в удовлетворении растущих потребностей общества.\"',\n",
       " '\"Все жертвы получили легкие ранения в результате аварии.\"; \"Спасатели были доставлены в больницу для посещения жертв.\"; \"Раненые вскоре оправятся от полученных ранений.\"',\n",
       " '\"Тигр Вудс был смешанным черным, китайским и коренным американцем.\"; \"Тигр Вудс не был первым азиатским американцем, выигравшим Турнир.\"; \"Достичь Тайгера Вудса было потрясающе, потому что он был самым молодым игроком в гольф.\"',\n",
       " '\"Ты должен следовать рутине даже по выходным.\"; \"Время для упражнений имеет важное значение.\"; \"Не вздремни долго, даже если ты очень хочешь спать.\"',\n",
       " '\"Потеря памяти неизбежна.\"; \"Светильность неразумна.\"; \"Мышечные мышцы не нуждаются в упражнениях.\"',\n",
       " '\"Клиенты в Америке, как и другие, используют вспышки, когда едят.\"; \"Американское правительство обсуждает проблему пищевойстаграмации.\"; \"Люди в Китае пытаются придумать идею для решения этой проблемы.\"',\n",
       " '\"Иногда правительство лжет, потому что оно должно отвечать общественным интересам.\"; \"Доктора считают, что если они лгут, то больные серьёзно больных выздоровеют быстрее.\"; \"Многие пациенты не хотят знать правду, особенно о серьезных заболеваниях.\"',\n",
       " '\"Крокодил напал на него, когда мальчик и его мать плавали.\"; \"Крокодил укусил руки мальчика, когда он достиг его.\"; \"За каждым шрамом всегда есть интересная история.\"',\n",
       " '\"Для сокращения люди ничего не могут с этим поделать.\"; \"Если кто-то ранен, он станет короче.\"; \"У женщин меньше и легче костей, чем у мужчин.\"',\n",
       " '\"Однажды компьютер полностью заменит людей.\"; \"В наше время компьютер является единственным самым важным изобретением.\"; \"В будущем мы можем использовать компьютер для всего.\"',\n",
       " '\"Вы можете увидеть в прямом эфире Peking Opera, наслаждаясь китайским традиционным чаем.\"; \"Тебе не нужно беспокоиться о языковой проблеме, чтобы посетить Таичи, если ты англичанин.\"; \"Китайский класс рисования чернил и воды, вероятно, открыт для обучения за пределами Китая.\"',\n",
       " '\"Американцы всегда легко получали доступ к Интернету.\"; \"Мировой телевизор к 2013 году составит 150 миллионов.\"; \"В 2005 году 45% семей в развивающихся странах имели телевизор.\"']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04d11300-92a9-4c39-b824-f206a0ba0b60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"В пустыне нет живых существ.\"; \"Люди могут делать пустыни.\"; \"В Иране многие песчаные районы находятся в пределах нефтяной стены.\"',\n",
       " '\"Нельсон Мандела был знаменитой фигурой международной славы.\"; \"Нельсон Мандела любил бокс, потому что он был боксером.\"; \"Нельсон Мандела изучал право в течение 50 лет.\"',\n",
       " '\"Летние программы могут помочь детям улучшить навыки математики.\"; \"Дети должны учиться математике в школе.\"; \"Учащиеся должны учиться математике в школе.\"',\n",
       " '\"Фестиваль Гластонбери проводится в течение трех дней.\"; \"Десятки тысяч молодых людей в Великобритании посещают фестиваль Гластонбери.\"; \"Фестиваль Гластонбери был самым популярным в Великобритании фестивалем.\"',\n",
       " '\"Учебник находится в центре города.\"; \"Учебник Котеборга расположен в центре города.\"; \"Промышленность и торговля в Котеборгском университете являются популярными.\"',\n",
       " '\"Бекки Смит сломала обе ноги и сломала лодыжку.\"; \"Спасатели спасли Мэллори Смит и Меган.\"; \"Все дети в машине погибли в аварии.\"',\n",
       " '\"Тигер Вудс родился в Калифорнии.\"; \"До 1990 года все белые и черные были нездоровы.\"; \"Мишель Ви выразила свою решимость играть на всех турнирах.\"',\n",
       " '\"Не двигайся, если хочешь спать.\"; \"Спать не нужно, если ты не спишь.\"; \"Перекуси, если хочешь спать эффективно.\"',\n",
       " '\"Ученые упражняться в своих умственных способностях.\"; \"Ученые используют лугосити, чтобы упражняться.\"; \"Люди должны упражняться перед экзаменами.\"',\n",
       " '\"В некоторых ресторанах Испании фотографируют свою еду.\"; \"В некоторых ресторанах Испании даже запрещают фотографирование продуктов питания.\"; \"В некоторых ресторанах Испании фотографируют только еду.\"',\n",
       " '\"Врачи всегда могут быть честными.\"; \"Обвинение в медицине помогает предотвратить преступление.\"; \"Врачи должны сказать пациенту правду о его болезни.\"',\n",
       " '\"Мать была очень страстна, чтобы помочь мальчику.\"; \"Крокодил схватил мальчика за ноги.\"; \"Мальчик едва ли пострадал от крокодила.\"',\n",
       " '\"Мы не такие высокие, как мужчины, потому что сжимаемся слишком часто.\"; \"С увеличением костей кости становятся меньше и легче.\"; \"Наши ноги в суставах сжимаются, потому что они меньше сжимаются.\"',\n",
       " '\"Компьютер - это очень дешевый способ поддерживать связь с друзьями и семьей.\"; \"Люди не разрешают своим детям пользоваться Интернетом, потому что он может быть очень полезен.\"; \"Компьютер - это очень хороший способ поддерживать связь с друзьями и семьей.\"',\n",
       " '\"Тебе нужно научиться рисовать с английским рассказом в театре Дяньцяо.\"; \"Плата за чашку чая в театре Дяньцяо Гарден стоит 60 юаней.\"; \"В театре Дяньцяо Хоутонг проводятся акробатические шоу, организованные китайской культурой.\"',\n",
       " '\"Телевидение оказывает позитивное влияние на жизнь людей во всем мире.\"; \"Люди в развивающихся странах не имеют доступа к Интернету.\"; \"Природные ресурсы не имеют ничего общего с насилием, избыточным весом и одиночеством.\"']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ecba4301-9983-4035-a63f-bb28146f6964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['В пустыне нет живых существ.',\n",
       " 'Нельсон Мандела изучал этот закон без перерыва в течение 50 лет.',\n",
       " 'Летние программы могут помочь детям.',\n",
       " 'Билеты на Фестиваль Гластонбери 2004 года были очень востребованы, несмотря на высокую цену.',\n",
       " 'Культурная жизнь университета очень богата.',\n",
       " 'К счастью, никто не получил слишком серьезных травм во время аварии.',\n",
       " 'Появление Тайгера Вудса на сцене гольфа изменило отношение к этому спорту в США.',\n",
       " 'Ложись спать сразу после горячей ванны.',\n",
       " 'Мозги нужно упражняться.',\n",
       " 'Рестораны в Испании думают о метафоре, чтобы удовлетворить потребности людей.',\n",
       " 'В некоторых случаях правдивая информация помогает пациентам справляться со своей болезнью.',\n",
       " 'Это фермер застрелил крокодила.',\n",
       " 'Мы не такие высокие в конце дня, как в начале.',\n",
       " 'С помощью компьютера, используемого в нашей повседневной жизни, мы можем сделать кое-что попроще, чем раньше.',\n",
       " 'Вы можете наслаждаться акробатическими шоу каждый день в 19 часов на открытом воздухе.',\n",
       " 'Более двух третей семей в мире будут иметь телевизор к 2013 году.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5218caca-aca9-4f9e-af57-fce24626ac6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': {'bleu': 0.19059422675805282,\n",
       "  'precisions': [0.4160714285714286,\n",
       "   0.2555147058823529,\n",
       "   0.1571969696969697,\n",
       "   0.087890625],\n",
       "  'brevity_penalty': 0.9735698412824109,\n",
       "  'length_ratio': 0.9739130434782609,\n",
       "  'translation_length': 560,\n",
       "  'reference_length': 575},\n",
       " 'sbleu': {'score': 19.059422675805287,\n",
       "  'counts': [233, 139, 83, 45],\n",
       "  'totals': [560, 544, 528, 512],\n",
       "  'precisions': [41.607142857142854,\n",
       "   25.551470588235293,\n",
       "   15.719696969696969,\n",
       "   8.7890625],\n",
       "  'bp': 0.9735698412824109,\n",
       "  'sys_len': 560,\n",
       "  'ref_len': 575},\n",
       " 'rouge': {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0},\n",
       " 'meteor': {'meteor': 0.40420596138295706}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics(predict_model, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07511fd2-18f9-4ce0-8d0a-d00a1c0a70b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_101743/4121503171.py:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for i in tqdm_notebook(range(N_STEPS), total=N_STEPS):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5068826fea948f4942a8210f880416c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BATCH_SIZE = 1\n",
    "N_STEPS = (len(tf_dataset_val) // BATCH_SIZE) + 1\n",
    "\n",
    "metrics_val = []\n",
    "\n",
    "for i in tqdm_notebook(range(N_STEPS), total=N_STEPS):\n",
    "    slice = tf_dataset_val[i*BATCH_SIZE:(i+1)*BATCH_SIZE]\n",
    "\n",
    "    if slice[\"inp\"]:\n",
    "        output = get_metric_inputs_seq2seq(slice[\"inp\"], model, tokenizer)\n",
    "\n",
    "        distractors = [item.replace('\\n', '').replace('  ',' ').replace('  ',' ').strip() for item in slice[\"distractors\"]]\n",
    "\n",
    "        metric = compute_metrics(output, distractors)\n",
    "\n",
    "        # код далее подходит только для батчей из одиночных примеров (BATCH_SIZE=1):\n",
    "        if \"ВОПРОС: Какое высказывание НЕ СООТВЕТСТВУЕТ тексту?\" in slice[\"inp\"][0]:\n",
    "            question = \"ВОПРОС: Какое высказывание НЕ СООТВЕТСТВУЕТ тексту?\"\n",
    "        else:\n",
    "            question = \"ВОПРОС: Какое высказывание СООТВЕТСТВУЕТ тексту? \"\n",
    "        metrics_val.append({\n",
    "            \"article\": slice[\"article_ru\"][0],\n",
    "            \"right_answer\": slice[\"right_answer\"][0],\n",
    "            \"question\": question,\n",
    "            \"distractors\": distractors[0],\n",
    "            \"output\": output[0],\n",
    "\n",
    "            \"bleu\": metric[\"bleu\"][\"bleu\"],\n",
    "            \"sbleu\": metric[\"sbleu\"][\"score\"],\n",
    "            \"rouge1\": metric[\"rouge\"][\"rouge1\"],\n",
    "            \"rouge2\": metric[\"rouge\"][\"rouge2\"],\n",
    "            \"rougeL\": metric[\"rouge\"][\"rougeL\"],\n",
    "            \"rougeLsum\": metric[\"rouge\"][\"rougeLsum\"],\n",
    "            \"meteor\": metric[\"meteor\"][\"meteor\"],\n",
    "\n",
    "            \"article_orig\": slice[\"article\"][0],\n",
    "            \"question_orig\": slice[\"question\"][0],\n",
    "            \"options_orig\": slice[\"options\"][0],\n",
    "            \"right_answer_orig\": slice[\"answer\"][0]\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "203bc33a-e8d9-47e9-9f10-c06f4993f6ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'article': '\"Тебе что-нибудь еще нужно, дорогая?\" Мой отец попросил меня, когда он положил мне в руки три двадцать долларовых купюр. Я ехал домой из семейного визита, и после того, как я позавтракал и наполнил машину газом, было очевидно, что мой отец хотел убедиться, что я буду в порядке на дороге.\\n\"Нет, папа. Ты уже столько сделал. Спасибо!\" Я снова была потрясена его подобными действиями по предоставлению всего, что мне нужно, хотя мне исполнилось 40. Но я понимаю, что в глазах отца я всегда буду его маленькой девочкой. Ему доставляет большое удовольствие знать, что его дети в порядке. Теперь, когда у него достаточно денег, он любит давать, когда в этом есть необходимость.\\nНо это не всегда так. Развёлся с моей матерью, когда мне было 11, мой отец не мог быть рядом со своими детьми так часто, как хотел бы. Деньги были также жесткими; даже посещения в выходные были редкими. Тем не менее, мой отец оставался в постоянном общении с нами и следил за тем, чтобы он был вовлечен в нашу жизнь. Хотя он не всегда мог быть там лично, я знала, что он всего лишь звонок. Я всегда мог быть уверен в этом.\\nДаже сейчас, почти 30 лет спустя, я дорожу тем, что могу взять трубку и позвонить папе, и он будет рядом со мной. У меня замечательный муж, но это не изменило того, как папа видит меня. Я все еще его ребёнок, и он любит видеть, что мои потребности удовлетворены.\\nЯ помню время, когда я ходил за покупками в хозяйственный магазин с папой. Я упомянула о своих планах нарисовать одну стену в моем доме. Ну, это все, что нужно, чтобы папа принял меры. К тому времени, как я добрался до линии выписки, все припасы, которые я выбирал, были выброшены из моих рук и помещены кое-какими вещами, которые он купил.\\nПотом было время, когда я взял его с собой, чтобы купить продукты для нескольких \"веществ\". К тому времени, как мы закончили, в моей тележке было полно продуктов с каждой полки в магазине! Мы с сестрой шутим, что если ты не хочешь, чтобы папа купил его для тебя, не упоминай даже, что ты чего-то хочешь.',\n",
       " 'right_answer': 'После того как автор вышла замуж, ее отец больше не вмешивается в ее жизнь.',\n",
       " 'question': 'ВОПРОС: Какое высказывание НЕ СООТВЕТСТВУЕТ тексту?',\n",
       " 'distractors': '\"Теперь экономическое положение отца является удовлетворительным.\"; \"Отец автора была разведена, когда ей было 11 лет.\"; \"У отца отличное чувство ответственности.\"',\n",
       " 'output': '\"Автор выразила благодарность своему отцу, когда она вышла замуж.\"; \"Отец автора часто посещал магазин, когда ей было 11.\"; \"Автор знала, что ее отец любит ее, но не знала, что она может дать ему что-нибудь еще.\"',\n",
       " 'bleu': 0.21424368511545036,\n",
       " 'sbleu': 21.424368511545037,\n",
       " 'rouge1': 1.0,\n",
       " 'rouge2': 0.0,\n",
       " 'rougeL': 1.0,\n",
       " 'rougeLsum': 1.0,\n",
       " 'meteor': 0.5076659747259551,\n",
       " 'article_orig': '\"Is there anything else you need, honey?\" my dad asked me as he put three twenty dollar bills in my hand. I was traveling back home from a family visit, and after treating me to breakfast and filling my car with gas, it was obvious that my dad wanted to make sure that I would be okay on the road.\\n\"No, Dad. You\\'ve done so much already. Thank you!\" I was overwhelmed once again by his kind acts of providing everything I needed, although I turned 40. Yet I realize that in my father\\'s eyes, I will always be his little girl. He takes deep pleasure in knowing his children are all right. Now that he has enough money, he loves to give whenever he sees a need.\\nBut this was not always the case. Divorced from my mother when I was 11, my dad couldn\\'t be around his kids as often as he would have liked. Money was also tight; even weekend visits were rare. However, my dad stayed in constant communication with us and made sure he was involved in our lives. Though he couldn\\'t always be there in person, I knew he was only a phone call away. I could always make sure of that.\\nEven now, almost 30 years later, I treasure knowing that I can pick up the phone and call Dad, and he\\'ll be there for me. I have a wonderful husband, but that hasn\\'t changed how Dad sees me. I\\'m still his child and he loves to see that my needs are met.\\nI remember a time when I was shopping in a hardware store   with Dad. I mentioned my plans to paint one wall in my house. Well, that\\'s all it took for Dad to take action. By the time I got to the checkout  line, all the supplies I picked out were put out of my hands and placed with things he bought.\\nThen there was the time when I took him with me to do some grocery shopping for just a few \" items\". By the time we were finished, my shopping cart was full of groceries from every shelf in the store! My sister and I joke that if you don\\'t want Dad to buy it for you, avoid even mentioning you want something.',\n",
       " 'question_orig': 'Which of the following is not TRUE?',\n",
       " 'options_orig': ['After the author got married, her father no longer get himself involved in her life.',\n",
       "  \"Now the father's economic condition is satisfying.\",\n",
       "  \"The author's father was divorced when she was 11.\",\n",
       "  'The father has a great sense of responsibility.'],\n",
       " 'right_answer_orig': 'A'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c0aaa914-2a22-49bb-818f-e22c1d469b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_val = pd.DataFrame(metrics_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c3daf446-bcf0-45e5-8221-c0fa0af118c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bleu</th>\n",
       "      <th>sbleu</th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "      <th>meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>175.000000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>175.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.241878</td>\n",
       "      <td>24.229969</td>\n",
       "      <td>0.034549</td>\n",
       "      <td>0.006061</td>\n",
       "      <td>0.032263</td>\n",
       "      <td>0.032263</td>\n",
       "      <td>0.448407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.206379</td>\n",
       "      <td>20.595737</td>\n",
       "      <td>0.162295</td>\n",
       "      <td>0.075705</td>\n",
       "      <td>0.154061</td>\n",
       "      <td>0.154061</td>\n",
       "      <td>0.168714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.378207</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.214145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.136922</td>\n",
       "      <td>13.692210</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.348858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.162145</td>\n",
       "      <td>16.214528</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.232998</td>\n",
       "      <td>23.299849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.478868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             bleu       sbleu      rouge1      rouge2      rougeL   rougeLsum  \\\n",
       "count  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000   \n",
       "mean     0.241878   24.229969    0.034549    0.006061    0.032263    0.032263   \n",
       "std      0.206379   20.595737    0.162295    0.075705    0.154061    0.154061   \n",
       "min      0.000000    7.378207    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.136922   13.692210    0.000000    0.000000    0.000000    0.000000   \n",
       "50%      0.162145   16.214528    0.000000    0.000000    0.000000    0.000000   \n",
       "75%      0.232998   23.299849    0.000000    0.000000    0.000000    0.000000   \n",
       "max      1.000000  100.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "           meteor  \n",
       "count  175.000000  \n",
       "mean     0.448407  \n",
       "std      0.168714  \n",
       "min      0.214145  \n",
       "25%      0.348858  \n",
       "50%      0.400267  \n",
       "75%      0.478868  \n",
       "max      0.999997  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_val.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a3441daf-001f-40d2-b62c-bf1e9f3e721d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_val.to_excel(\"T5Metrics-TF-val.xlsx\", engine=\"openpyxl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
