{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bafc561-654b-4e23-a49f-c9fd1a6e43a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import evaluate\n",
    "import torch as tt\n",
    "import pandas as pd\n",
    "\n",
    "from typing import Any, Dict, Union\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tqdm import tqdm_notebook\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, PreTrainedModel, PreTrainedTokenizer\n",
    "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9a4a8fe-e042-4861-9b22-7f38059b03ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/usr/lib/python3/dist-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[nltk_data] Downloading package wordnet to /home/user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/user/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# models:\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"ai-forever/ruT5-base\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"ai-forever/ruT5-base\").to(tt.device(\"cuda:0\"))\n",
    "\n",
    "# metrics:\n",
    "bleu4 = evaluate.load(\"bleu\")\n",
    "sbleu = evaluate.load(\"sacrebleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "meteor = evaluate.load(\"meteor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e77ddc5d-fb1b-4859-a1b1-5b339acb3248",
   "metadata": {},
   "outputs": [],
   "source": [
    "option_id_dict = {\n",
    "    'A': 0, 'B': 1, 'C': 2, 'D': 3\n",
    "}\n",
    "\n",
    "def to_new_format(example: dict[str, Union[str, list[str]]]) -> str:\n",
    "  inp, label = '', ''\n",
    "  example[\"options_ru\"] = [option for option in example[\"options_ru\"] if option]\n",
    "  right_answer = example['options_ru'][option_id_dict[example['answer']]]\n",
    "\n",
    "  right_answer = right_answer.replace('\"', \"'\")\n",
    "\n",
    "  qtext_orig = example[\"question\"].lower()\n",
    "  if (\"not true\" in qtext_orig) or (\"false\" in qtext_orig) or (\"n't true\" in qtext_orig) or (\"untrue\" in qtext_orig):\n",
    "      if (\"not false\" in qtext_orig) or (\"n't false\" in qtext_orig):\n",
    "          inp += example['article_ru'] + \" \" + \"ВОПРОС: Какое высказывание СООТВЕТСТВУЕТ тексту? \"\n",
    "      else:\n",
    "          inp += example['article_ru'] + \" \" + \"ВОПРОС: Какое высказывание НЕ СООТВЕТСТВУЕТ тексту? \"\n",
    "  else:\n",
    "      inp += example['article_ru'] + \" \" + \"ВОПРОС: Какое высказывание СООТВЕТСТВУЕТ тексту? \"\n",
    "\n",
    "  inp += f'ПРАВИЛЬНЫЙ ОТВЕТ: \"{right_answer}\".'\n",
    "  inp += 'НЕПРАВИЛЬНЫЕ ВАРИАНТЫ ОТВЕТА: '\n",
    "\n",
    "  options = example[\"options_ru\"]\n",
    "  options = [\n",
    "      option.replace('\"', \"'\") for option in options if option != right_answer\n",
    "  ]\n",
    "  options = [\n",
    "      f'\"{option}\"' for option in options\n",
    "  ]\n",
    "  label = \"; \".join(options)\n",
    "\n",
    "  distractors_len = len(tokenizer(label)[\"input_ids\"])\n",
    "    \n",
    "  return {\"inp\": inp, \"right_answer\": right_answer, \"distractors\": label, \"distractors_len\": distractors_len}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b03d6cee-05ee-40a2-9860-f80bc20828e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4cf606fd1e04ce49fb4f5660020792a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3288 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad1fd666a9e541ecb4031ef81b29c879",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/175 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e1fe551510b45c08cfb9ae74f97bdf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/187 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"tf_dataset_pretty_filtered.json\", 'r', encoding=\"utf8\") as inp:\n",
    "    tf_dataset = json.load(inp)\n",
    "\n",
    "tf_dataset_train, tf_dataset_val, tf_dataset_test = tf_dataset[\"train\"], tf_dataset[\"val\"], tf_dataset[\"test\"]\n",
    "tf_dataset_train = Dataset.from_list(tf_dataset_train)\n",
    "tf_dataset_val = Dataset.from_list(tf_dataset_val)\n",
    "tf_dataset_test = Dataset.from_list(tf_dataset_test)\n",
    "\n",
    "tf_dataset_train = tf_dataset_train.map(to_new_format)\n",
    "tf_dataset_val = tf_dataset_val.map(to_new_format)\n",
    "tf_dataset_test = tf_dataset_test.map(to_new_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cdb0e2e-852a-4971-86cb-97a4629e19cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    3288.000000\n",
       "mean       47.546533\n",
       "std        11.146655\n",
       "min        21.000000\n",
       "25%        40.000000\n",
       "50%        46.000000\n",
       "75%        54.000000\n",
       "max       136.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(tf_dataset_train[\"distractors_len\"]).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de702f29-456c-454e-a17e-c2ce042f3b01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LEN = pd.Series(tf_dataset_train[\"distractors_len\"]).quantile(0.99)\n",
    "MAX_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a31a9d34-3a7d-4935-b715-490af7b63f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(text: str, model: PreTrainedModel) -> str:\n",
    "    input_ = tokenizer([text], return_tensors=\"pt\")\n",
    "    output = model.generate(\n",
    "        input_[\"input_ids\"].to(tt.device(\"cuda:0\"))\n",
    "    )\n",
    "    return tokenizer.batch_decode(output)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cab0fc91-1bcb-415b-8f68-42f4b83c02ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Почему люди скрещивают пальцы, сталкиваясь с трудностями нерелигиозных людей? Почему мы виним чёрного кота, когда видим низкий балл теста?\n",
      "Эти привычки называются волшебным мышлением и можно найти везде в повседневной жизни. Например, человек хочет чего - то плохого для раздражающего коллеги. Или родители молятся о безопасном бою, когда их сын уходит. \n",
      "Психологическая помощь\n",
      "Изучая эти привычки, ученые решили найти причину, по которой люди верят в магические силы. Психологи имеют свой собственный подход к этому вопросу. \n",
      "\"Я думаю, что отчасти это потому, что мы постоянно подвергаемся воздействию наших собственных мыслей, и поэтому, скорее всего, переоценим их связь с внешними событиями. \"Сказала Эмили Пронин, психолог в префиксе = st1 /US.\n",
      "Для людей, которые не уверены в своих способностях или медленных действиях, волшебное мышление может быть большой помощью, объяснил доктор Дэниел Вегнер, профессор психологии Гарварда. Чувство, что их собственные мысли могут контролировать вещи, может помочь людям бороться с депрессией. \n",
      "Предательство, приобретенное в результате эволюции\n",
      "Но у эволюционистов есть свои собственные идеи. \n",
      "Две школы эволюционного мышления придумали причины, анализируя развитие человека. Они являются « адаптационными » и « теоретиками побочных продуктов ». \n",
      "Адаптационисты говорят, что вера в магические силы заставляет людей чувствовать себя лучше, меньше беспокоиться о трудностях, больше фокусироваться на будущем и больше заботиться о себе. У людей с сильной верой (в чем-то) больше шансов выжить в трудной среде или соперничестве. \n",
      "Но в теории побочных продуктов магическое мышление является продуктом наших психологических особенностей. \n",
      "\"Мы автоматически ищем объяснение того, почему что-то происходит,\" объясняет Джастин Барретт, психолог, мозг, таким образом, эволюционировал, чтобы быстро судить о причинно-следственной связи. Так что мы часто связываем два события, основываясь не более чем на совпадении. Например, « Я просто думала о том, чтобы найти свою старую школьную подружку, когда она вдруг позвонила мне ».\n",
      "Еще одна психологическая особенность — это теория разума. Мы признаем, что может существовать невидимая сила ума, которая влияет на исход инцидента. \n",
      "Теоретики побочных продуктов утверждают, что из-за этих особенностей мы рождаемся с тенденцией верить в наше волшебное мышление. \n",
      "Но нам нужно ограничить наше волшебное мышление, предупредить психологов. \n",
      "Для большинства людей убеждения - это просто утешительный частный ритуал. Когда на карту поставлено что-то важное, например испытание, представление или отношения, люди не просто совершают свои личные ритуалы. Они должны подготовиться. ВОПРОС: Какое высказывание СООТВЕТСТВУЕТ тексту? ПРАВИЛЬНЫЙ ОТВЕТ: \"Доктор Дэниел Вегнер считает, что магическое мышление может помочь.\".НЕПРАВИЛЬНЫЕ ВАРИАНТЫ ОТВЕТА: \n"
     ]
    }
   ],
   "source": [
    "print(tf_dataset_test[42][\"inp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff58be34-a643-490d-96c4-f2770bbda765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Джастин Барретт, эволюционист, отрицает, что магическое мышление является продуктом адаптации.\"; \"У людей со слабой верой больше шансов выжить в соперничестве.\"; \"Адаптационисты думают, что мы рождаемся с тенденцией верить в наше волшебное мышление.\"\n"
     ]
    }
   ],
   "source": [
    "print(tf_dataset_test[42][\"distractors\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75f754c2-9ccd-4db8-985d-675aa698a5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> <extra_id_0>,,,,,,,,,,,,,,,,,,\n"
     ]
    }
   ],
   "source": [
    "predict_model = model_predict(tf_dataset_test[42][\"inp\"], model)\n",
    "print(predict_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65e6d37f-d061-44f0-bea1-9fdb351726b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "У тебя есть какие-нибудь замечательные планы на предстоящие зимние каникулы? Вот замечательные фильмы, чтобы убить время. ВОПРОС: Какое высказывание СООТВЕТСТВУЕТ тексту? ПРАВИЛЬНЫЙ ОТВЕТ: \"Сиротка - это комедия об Эстер, усыновленной доброй семьёй.\".НЕПРАВИЛЬНЫЕ ВАРИАНТЫ ОТВЕТА: \n"
     ]
    }
   ],
   "source": [
    "print(tf_dataset_test[100][\"inp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd2243a7-765e-4eb0-b878-02b99a9aee4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"500 дней лета - роман с счастливым концом.\"; \"Мы идем показывать красивые пейзажи во время путешествия молодых пар.\"; \"Древнее предсказание майя произойдет в 2012 году.\"\n"
     ]
    }
   ],
   "source": [
    "print(tf_dataset_test[100][\"distractors\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5de933df-0f05-4a50-8ad7-c05a6bc11c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> <extra_id_0> :  :::::::::::::::\n"
     ]
    }
   ],
   "source": [
    "predict_model = model_predict(tf_dataset_test[100][\"inp\"], model)\n",
    "print(predict_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "979b414b-cdeb-4d86-a363-9df4ab25ebdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> <extra_id_0> :  :::::::::::::::\n"
     ]
    }
   ],
   "source": [
    "predict_model = model_predict(tf_dataset_test[100][\"inp\"], model)\n",
    "print(predict_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7e0bcd7-c20e-4dd8-9518-b59c0650c72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method generate in module transformers.generation.utils:\n",
      "\n",
      "generate(inputs: Optional[torch.Tensor] = None, generation_config: Optional[transformers.generation.configuration_utils.GenerationConfig] = None, logits_processor: Optional[transformers.generation.logits_process.LogitsProcessorList] = None, stopping_criteria: Optional[transformers.generation.stopping_criteria.StoppingCriteriaList] = None, prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], List[int]]] = None, synced_gpus: Optional[bool] = None, assistant_model: Optional[ForwardRef('PreTrainedModel')] = None, streamer: Optional[ForwardRef('BaseStreamer')] = None, negative_prompt_ids: Optional[torch.Tensor] = None, negative_prompt_attention_mask: Optional[torch.Tensor] = None, **kwargs) -> Union[transformers.generation.utils.GenerateDecoderOnlyOutput, transformers.generation.utils.GenerateEncoderDecoderOutput, transformers.generation.utils.GenerateBeamDecoderOnlyOutput, transformers.generation.utils.GenerateBeamEncoderDecoderOutput, torch.LongTensor] method of transformers.models.t5.modeling_t5.T5ForConditionalGeneration instance\n",
      "    Generates sequences of token ids for models with a language modeling head.\n",
      "    \n",
      "    <Tip warning={true}>\n",
      "    \n",
      "    Most generation-controlling parameters are set in `generation_config` which, if not passed, will be set to the\n",
      "    model's default generation configuration. You can override any `generation_config` by passing the corresponding\n",
      "    parameters to generate(), e.g. `.generate(inputs, num_beams=4, do_sample=True)`.\n",
      "    \n",
      "    For an overview of generation strategies and code examples, check out the [following\n",
      "    guide](../generation_strategies).\n",
      "    \n",
      "    </Tip>\n",
      "    \n",
      "    Parameters:\n",
      "        inputs (`torch.Tensor` of varying shape depending on the modality, *optional*):\n",
      "            The sequence used as a prompt for the generation or as model inputs to the encoder. If `None` the\n",
      "            method initializes it with `bos_token_id` and a batch size of 1. For decoder-only models `inputs`\n",
      "            should be in the format of `input_ids`. For encoder-decoder models *inputs* can represent any of\n",
      "            `input_ids`, `input_values`, `input_features`, or `pixel_values`.\n",
      "        generation_config (`~generation.GenerationConfig`, *optional*):\n",
      "            The generation configuration to be used as base parametrization for the generation call. `**kwargs`\n",
      "            passed to generate matching the attributes of `generation_config` will override them. If\n",
      "            `generation_config` is not provided, the default will be used, which has the following loading\n",
      "            priority: 1) from the `generation_config.json` model file, if it exists; 2) from the model\n",
      "            configuration. Please note that unspecified parameters will inherit [`~generation.GenerationConfig`]'s\n",
      "            default values, whose documentation should be checked to parameterize generation.\n",
      "        logits_processor (`LogitsProcessorList`, *optional*):\n",
      "            Custom logits processors that complement the default logits processors built from arguments and\n",
      "            generation config. If a logit processor is passed that is already created with the arguments or a\n",
      "            generation config an error is thrown. This feature is intended for advanced users.\n",
      "        stopping_criteria (`StoppingCriteriaList`, *optional*):\n",
      "            Custom stopping criteria that complements the default stopping criteria built from arguments and a\n",
      "            generation config. If a stopping criteria is passed that is already created with the arguments or a\n",
      "            generation config an error is thrown. If your stopping criteria depends on the `scores` input, make\n",
      "            sure you pass `return_dict_in_generate=True, output_scores=True` to `generate`. This feature is\n",
      "            intended for advanced users.\n",
      "        prefix_allowed_tokens_fn (`Callable[[int, torch.Tensor], List[int]]`, *optional*):\n",
      "            If provided, this function constraints the beam search to allowed tokens only at each step. If not\n",
      "            provided no constraint is applied. This function takes 2 arguments: the batch ID `batch_id` and\n",
      "            `input_ids`. It has to return a list with the allowed tokens for the next generation step conditioned\n",
      "            on the batch ID `batch_id` and the previously generated tokens `inputs_ids`. This argument is useful\n",
      "            for constrained generation conditioned on the prefix, as described in [Autoregressive Entity\n",
      "            Retrieval](https://arxiv.org/abs/2010.00904).\n",
      "        synced_gpus (`bool`, *optional*):\n",
      "            Whether to continue running the while loop until max_length. Unless overridden this flag will be set to\n",
      "            `True` under DeepSpeed ZeRO Stage 3 multiple GPUs environment to avoid hanging if one GPU finished\n",
      "            generating before other GPUs. Otherwise it'll be set to `False`.\n",
      "        assistant_model (`PreTrainedModel`, *optional*):\n",
      "            An assistant model that can be used to accelerate generation. The assistant model must have the exact\n",
      "            same tokenizer. The acceleration is achieved when forecasting candidate tokens with the assistent model\n",
      "            is much faster than running generation with the model you're calling generate from. As such, the\n",
      "            assistant model should be much smaller.\n",
      "        streamer (`BaseStreamer`, *optional*):\n",
      "            Streamer object that will be used to stream the generated sequences. Generated tokens are passed\n",
      "            through `streamer.put(token_ids)` and the streamer is responsible for any further processing.\n",
      "        negative_prompt_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "            The negative prompt needed for some processors such as CFG. The batch size must match the input batch\n",
      "            size. This is an experimental feature, subject to breaking API changes in future versions.\n",
      "        negative_prompt_attention_mask (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "            Attention_mask for `negative_prompt_ids`.\n",
      "        kwargs (`Dict[str, Any]`, *optional*):\n",
      "            Ad hoc parametrization of `generation_config` and/or additional model-specific kwargs that will be\n",
      "            forwarded to the `forward` function of the model. If the model is an encoder-decoder model, encoder\n",
      "            specific kwargs should not be prefixed and decoder specific kwargs should be prefixed with *decoder_*.\n",
      "    \n",
      "    Return:\n",
      "        [`~utils.ModelOutput`] or `torch.LongTensor`: A [`~utils.ModelOutput`] (if `return_dict_in_generate=True`\n",
      "        or when `config.return_dict_in_generate=True`) or a `torch.LongTensor`.\n",
      "    \n",
      "            If the model is *not* an encoder-decoder model (`model.config.is_encoder_decoder=False`), the possible\n",
      "            [`~utils.ModelOutput`] types are:\n",
      "    \n",
      "                - [`~generation.GenerateDecoderOnlyOutput`],\n",
      "                - [`~generation.GenerateBeamDecoderOnlyOutput`]\n",
      "    \n",
      "            If the model is an encoder-decoder model (`model.config.is_encoder_decoder=True`), the possible\n",
      "            [`~utils.ModelOutput`] types are:\n",
      "    \n",
      "                - [`~generation.GenerateEncoderDecoderOutput`],\n",
      "                - [`~generation.GenerateBeamEncoderDecoderOutput`]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model.generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d178e98e-e2ea-4328-977f-636dedb14793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metric_inputs_seq2seq(\n",
    "    input_batch: list[str], #label_batch: list[str],\n",
    "    model: PreTrainedModel, tokenizer: PreTrainedTokenizer\n",
    ") -> list[str]:\n",
    "    input_batch_ = tokenizer(input_batch, return_tensors=\"pt\", padding=True)[\"input_ids\"].to(tt.device(\"cuda:0\"))\n",
    "    # label_batch_ = tokenizer(label_batch, return_tensors=\"pt\", padding=True)[\"input_ids\"]\n",
    "\n",
    "    # output_length = label_batch_.shape[-1]\n",
    "\n",
    "    with tt.no_grad():\n",
    "        output_batch = model.generate(input_batch_, max_length=MAX_LEN)\n",
    "\n",
    "    output = [\n",
    "        sent.replace(\"<pad>\", \" \").replace(\"</s>\", \" \").strip() for sent in tokenizer.batch_decode(output_batch) # \n",
    "    ]\n",
    "    \n",
    "    del input_batch_\n",
    "    del output_batch\n",
    "    # del label_batch_\n",
    "    tt.cuda.empty_cache()\n",
    "\n",
    "    return output\n",
    "\n",
    "def compute_metrics(output: list[str], label_batch: list[str]) -> dict:\n",
    "    metric_dict = {\n",
    "        \"bleu\": bleu4.compute(predictions=output, references=[[label] for label in label_batch]),\n",
    "        \"sbleu\": sbleu.compute(predictions=output, references=[[label] for label in label_batch]),\n",
    "        \"rouge\": rouge.compute(predictions=output, references=label_batch),\n",
    "        \"meteor\": meteor.compute(predictions=output, references=label_batch)\n",
    "    }\n",
    "    return metric_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae179de2-929f-4df7-adf6-d17318117596",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = tf_dataset_test[\"inp\"][:16]\n",
    "right_answers = tf_dataset_test[\"right_answer\"][:16]\n",
    "labels = [item.replace('\\n', '').replace('  ',' ').replace('  ',' ').strip() for item in tf_dataset_test[\"distractors\"][:16]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d727f17-ee18-4171-b071-aa76bc18c6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_model = get_metric_inputs_seq2seq(model_inputs, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd7c8153-42be-4533-ba00-6fab71357fcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"Самые великие земледелцы пустыни - это люди.\"; \"Пустыни быстро растут.\"; \"Размеры пустыни постоянно меняются.\"',\n",
       " '\"Нельсон Мандела не был его оригинальным именем.\"; \"Нельсон Мандела был назван своим учителем.\"; \"Нельсон Мандела основал свою собственную юридическую фирму до того, как получил степень юриста.\"',\n",
       " '\"Детям нехорошо веселиться летом.\"; \"Детям будет скучно читать программы\"; \"Учителям не нужно помогать детям анализировать уроки.\"',\n",
       " '\"Фестиваль Гластонбери работает на прибыльной основе.\"; \"Джеймс Браун и Джосс Стоун родились в бедных семьях.\"; \"В 1970 году на фестивале Гластонбери можно бесплатно пообедать на ферме.\"',\n",
       " '\"Университет находится в центре города.\"; \"Студенты могут жить на улице.\"; \"Университет не заинтересован в удовлетворении растущих потребностей общества.\"',\n",
       " '\"Все жертвы получили легкие ранения в результате аварии.\"; \"Спасатели были доставлены в больницу для посещения жертв.\"; \"Раненые вскоре оправятся от полученных ранений.\"',\n",
       " '\"Тигр Вудс был смешанным черным, китайским и коренным американцем.\"; \"Тигр Вудс не был первым азиатским американцем, выигравшим Турнир.\"; \"Достичь Тайгера Вудса было потрясающе, потому что он был самым молодым игроком в гольф.\"',\n",
       " '\"Ты должен следовать рутине даже по выходным.\"; \"Время для упражнений имеет важное значение.\"; \"Не вздремни долго, даже если ты очень хочешь спать.\"',\n",
       " '\"Потеря памяти неизбежна.\"; \"Светильность неразумна.\"; \"Мышечные мышцы не нуждаются в упражнениях.\"',\n",
       " '\"Клиенты в Америке, как и другие, используют вспышки, когда едят.\"; \"Американское правительство обсуждает проблему пищевойстаграмации.\"; \"Люди в Китае пытаются придумать идею для решения этой проблемы.\"',\n",
       " '\"Иногда правительство лжет, потому что оно должно отвечать общественным интересам.\"; \"Доктора считают, что если они лгут, то больные серьёзно больных выздоровеют быстрее.\"; \"Многие пациенты не хотят знать правду, особенно о серьезных заболеваниях.\"',\n",
       " '\"Крокодил напал на него, когда мальчик и его мать плавали.\"; \"Крокодил укусил руки мальчика, когда он достиг его.\"; \"За каждым шрамом всегда есть интересная история.\"',\n",
       " '\"Для сокращения люди ничего не могут с этим поделать.\"; \"Если кто-то ранен, он станет короче.\"; \"У женщин меньше и легче костей, чем у мужчин.\"',\n",
       " '\"Однажды компьютер полностью заменит людей.\"; \"В наше время компьютер является единственным самым важным изобретением.\"; \"В будущем мы можем использовать компьютер для всего.\"',\n",
       " '\"Вы можете увидеть в прямом эфире Peking Opera, наслаждаясь китайским традиционным чаем.\"; \"Тебе не нужно беспокоиться о языковой проблеме, чтобы посетить Таичи, если ты англичанин.\"; \"Китайский класс рисования чернил и воды, вероятно, открыт для обучения за пределами Китая.\"',\n",
       " '\"Американцы всегда легко получали доступ к Интернету.\"; \"Мировой телевизор к 2013 году составит 150 миллионов.\"; \"В 2005 году 45% семей в развивающихся странах имели телевизор.\"']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04d11300-92a9-4c39-b824-f206a0ba0b60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<extra_id_0> :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::',\n",
       " '<extra_id_0>,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,',\n",
       " '<extra_id_0>,, <extra_id_2>',\n",
       " '<extra_id_0>, в том числе, в мире.,,,,!!!!!! <extra_id_12> <extra_id_13>',\n",
       " '<extra_id_0>,, в, в в в в в в в в в в в в в в в в в в в в в в в в в в в в в в в в в в в в в в в в в в в в в в в в в в в в в в в в в в в в в в в в в в в в в в в в в в в',\n",
       " '<extra_id_0>, за, за, за, за, за, за, за.... <extra_id_1> <extra_id_2>',\n",
       " '<extra_id_0>,, в, в и, в, в и,, в и,, в и,, в',\n",
       " '<extra_id_0>, что мы, и, <extra_id_2>',\n",
       " '<extra_id_0> :, как и в',\n",
       " '<extra_id_0>, в и в',\n",
       " '<extra_id_0>, в,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,',\n",
       " '<extra_id_0>., в, в,,,,,,,,,!!!!!!!!!!!!!',\n",
       " '<extra_id_0>, что мы, <extra_id_2>',\n",
       " '<extra_id_0>. <extra_id_1>. <extra_id_2>',\n",
       " '<extra_id_0>,',\n",
       " '<extra_id_0> :) <extra_id_1> :) <extra_id_2>']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ecba4301-9983-4035-a63f-bb28146f6964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['В пустыне нет живых существ.',\n",
       " 'Нельсон Мандела изучал этот закон без перерыва в течение 50 лет.',\n",
       " 'Летние программы могут помочь детям.',\n",
       " 'Билеты на Фестиваль Гластонбери 2004 года были очень востребованы, несмотря на высокую цену.',\n",
       " 'Культурная жизнь университета очень богата.',\n",
       " 'К счастью, никто не получил слишком серьезных травм во время аварии.',\n",
       " 'Появление Тайгера Вудса на сцене гольфа изменило отношение к этому спорту в США.',\n",
       " 'Ложись спать сразу после горячей ванны.',\n",
       " 'Мозги нужно упражняться.',\n",
       " 'Рестораны в Испании думают о метафоре, чтобы удовлетворить потребности людей.',\n",
       " 'В некоторых случаях правдивая информация помогает пациентам справляться со своей болезнью.',\n",
       " 'Это фермер застрелил крокодила.',\n",
       " 'Мы не такие высокие в конце дня, как в начале.',\n",
       " 'С помощью компьютера, используемого в нашей повседневной жизни, мы можем сделать кое-что попроще, чем раньше.',\n",
       " 'Вы можете наслаждаться акробатическими шоу каждый день в 19 часов на открытом воздухе.',\n",
       " 'Более двух третей семей в мире будут иметь телевизор к 2013 году.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5218caca-aca9-4f9e-af57-fce24626ac6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': {'bleu': 0.0,\n",
       "  'precisions': [0.050243111831442464, 0.0, 0.0, 0.0],\n",
       "  'brevity_penalty': 1.0,\n",
       "  'length_ratio': 1.0730434782608695,\n",
       "  'translation_length': 617,\n",
       "  'reference_length': 575},\n",
       " 'sbleu': {'score': 0.1407467864723911,\n",
       "  'counts': [31, 0, 0, 0],\n",
       "  'totals': [617, 601, 585, 569],\n",
       "  'precisions': [5.024311183144246,\n",
       "   0.08319467554076539,\n",
       "   0.042735042735042736,\n",
       "   0.021968365553602813],\n",
       "  'bp': 1.0,\n",
       "  'sys_len': 617,\n",
       "  'ref_len': 575},\n",
       " 'rouge': {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0},\n",
       " 'meteor': {'meteor': 0.023340126465681454}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics(predict_model, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07511fd2-18f9-4ce0-8d0a-d00a1c0a70b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_103386/3942987125.py:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for i in tqdm_notebook(range(N_STEPS), total=N_STEPS):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa08ee326309476a8e83eb568357111f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BATCH_SIZE = 1\n",
    "N_STEPS = (len(tf_dataset_val) // BATCH_SIZE) + 1\n",
    "\n",
    "metrics_val = []\n",
    "\n",
    "for i in tqdm_notebook(range(N_STEPS), total=N_STEPS):\n",
    "    slice = tf_dataset_val[i*BATCH_SIZE:(i+1)*BATCH_SIZE]\n",
    "\n",
    "    if slice[\"inp\"]:\n",
    "        output = get_metric_inputs_seq2seq(slice[\"inp\"], model, tokenizer)\n",
    "\n",
    "        distractors = [item.replace('\\n', '').replace('  ',' ').replace('  ',' ').strip() for item in slice[\"distractors\"]]\n",
    "\n",
    "        if \"ВОПРОС: Какое высказывание НЕ СООТВЕТСТВУЕТ тексту?\" in slice[\"inp\"][0]:\n",
    "            question = \"ВОПРОС: Какое высказывание НЕ СООТВЕТСТВУЕТ тексту?\"\n",
    "        else:\n",
    "            question = \"ВОПРОС: Какое высказывание СООТВЕТСТВУЕТ тексту? \"\n",
    "\n",
    "        try:\n",
    "            metric = compute_metrics(output, distractors)\n",
    "            metrics_val.append({\n",
    "                \"article\": slice[\"article_ru\"][0],\n",
    "                \"right_answer\": slice[\"right_answer\"][0],\n",
    "                \"question\": question,\n",
    "                \"distractors\": distractors[0],\n",
    "                \"output\": output[0],\n",
    "    \n",
    "                \"bleu\": metric[\"bleu\"][\"bleu\"],\n",
    "                \"sbleu\": metric[\"sbleu\"][\"score\"],\n",
    "                \"rouge1\": metric[\"rouge\"][\"rouge1\"],\n",
    "                \"rouge2\": metric[\"rouge\"][\"rouge2\"],\n",
    "                \"rougeL\": metric[\"rouge\"][\"rougeL\"],\n",
    "                \"rougeLsum\": metric[\"rouge\"][\"rougeLsum\"],\n",
    "                \"meteor\": metric[\"meteor\"][\"meteor\"],\n",
    "    \n",
    "                \"article_orig\": slice[\"article\"][0],\n",
    "                \"question_orig\": slice[\"question\"][0],\n",
    "                \"options_orig\": slice[\"options\"][0],\n",
    "                \"right_answer_orig\": slice[\"answer\"][0]\n",
    "            })\n",
    "        except ZeroDivisionError:\n",
    "            metrics_val.append({\n",
    "                \"article\": slice[\"article_ru\"][0],\n",
    "                \"right_answer\": slice[\"right_answer\"][0],\n",
    "                \"question\": question,\n",
    "                \"distractors\": distractors[0],\n",
    "                \"output\": output[0],\n",
    "    \n",
    "                \"bleu\": 0,\n",
    "                \"sbleu\": 0,\n",
    "                \"rouge1\": 0,\n",
    "                \"rouge2\": 0,\n",
    "                \"rougeL\": 0,\n",
    "                \"rougeLsum\": 0,\n",
    "                \"meteor\": 0,\n",
    "    \n",
    "                \"article_orig\": slice[\"article\"][0],\n",
    "                \"question_orig\": slice[\"question\"][0],\n",
    "                \"options_orig\": slice[\"options\"][0],\n",
    "                \"right_answer_orig\": slice[\"answer\"][0]\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c0aaa914-2a22-49bb-818f-e22c1d469b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_val = pd.DataFrame(metrics_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c3daf446-bcf0-45e5-8221-c0fa0af118c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bleu</th>\n",
       "      <th>sbleu</th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "      <th>meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>175.0</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>175.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>175.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.484962</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.479414</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.418443</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.736774</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.030770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.053632</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.088933</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        bleu       sbleu  rouge1  rouge2  rougeL  rougeLsum      meteor\n",
       "count  175.0  175.000000   175.0   175.0   175.0      175.0  175.000000\n",
       "mean     0.0    0.484962     0.0     0.0     0.0        0.0    0.019417\n",
       "std      0.0    0.479414     0.0     0.0     0.0        0.0    0.018901\n",
       "min      0.0    0.000000     0.0     0.0     0.0        0.0    0.000000\n",
       "25%      0.0    0.000000     0.0     0.0     0.0        0.0    0.000000\n",
       "50%      0.0    0.418443     0.0     0.0     0.0        0.0    0.015060\n",
       "75%      0.0    0.736774     0.0     0.0     0.0        0.0    0.030770\n",
       "max      0.0    2.053632     0.0     0.0     0.0        0.0    0.088933"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_val.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a3441daf-001f-40d2-b62c-bf1e9f3e721d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_val.to_excel(\"T5Metrics-TF-Baseline-val.xlsx\", engine=\"openpyxl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c2991f-14a4-4058-9f6c-83a7a11fe11e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
