{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a112ccb3-a799-4ae3-9e50-f33c495cc737",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1fd261e-939f-454f-ac0b-bb6b21e2a455",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import evaluate\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, EvalPrediction\n",
    "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from typing import Any, Dict, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef47b2f3-333d-4202-b216-7a511e7ddb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/usr/lib/python3/dist-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"ai-forever/ruT5-base\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"ai-forever/ruT5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fca8ed4b-19f8-4b40-96e8-9ab505a89271",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/user/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# metrics:\n",
    "bleu4 = evaluate.load(\"bleu\")\n",
    "sbleu = evaluate.load(\"sacrebleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "meteor = evaluate.load(\"meteor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e34b844-6f36-493d-8098-f26c2c14c39b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ebcd10c-f28d-4205-8b7b-61144db35542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48d53025-7b71-4f6f-a950-3158a5a61876",
   "metadata": {},
   "outputs": [],
   "source": [
    "option_id_dict = {\n",
    "    'A': 0, 'B': 1, 'C': 2, 'D': 3\n",
    "}\n",
    "\n",
    "def to_new_format(example: dict[str, Union[str, list[str]]]) -> str:\n",
    "  inp, label = '', ''\n",
    "  example[\"options_ru\"] = [option for option in example[\"options_ru\"] if option]\n",
    "  right_answer = example['options_ru'][option_id_dict[example['answer']]]\n",
    "\n",
    "  right_answer = right_answer.replace('\"', \"'\")\n",
    "\n",
    "  inp += example['article_ru'] + \" \" + \"ВОПРОС: Какое название лучше всего подойдёт для этого текста? \"\n",
    "  inp += f'ПРАВИЛЬНЫЙ ОТВЕТ: \"{right_answer}\".'\n",
    "  inp += 'НЕПРАВИЛЬНЫЕ ВАРИАНТЫ ОТВЕТА: '\n",
    "\n",
    "  options = example[\"options_ru\"]\n",
    "  options = [\n",
    "      option.replace('\"', \"'\") for option in options if option != right_answer\n",
    "  ]\n",
    "  options = [\n",
    "      f'\"{option}\"' for option in options\n",
    "  ]\n",
    "  label = \"; \".join(options)\n",
    "    \n",
    "  return {\"inp\": inp, \"distractors\": label}\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"inp\"]\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        examples[\"distractors\"]\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44bdce1b-cead-42f9-8bc3-424a249431eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f708341528b483c934def4391d5ad4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4375 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dff9ede46410471e96fbc6358025af8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/219 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a082925093c42da972c4b1bd9813fe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/242 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"title_dataset_pretty_filtered.json\", 'r', encoding=\"utf8\") as inp:\n",
    "    title_dataset = json.load(inp)\n",
    "\n",
    "title_dataset_train, title_dataset_val, title_dataset_test = title_dataset[\"train\"], title_dataset[\"val\"], title_dataset[\"test\"]\n",
    "title_dataset_train = Dataset.from_list(title_dataset_train)\n",
    "title_dataset_val = Dataset.from_list(title_dataset_val)\n",
    "title_dataset_test = Dataset.from_list(title_dataset_test)\n",
    "\n",
    "title_dataset_train = title_dataset_train.map(to_new_format)\n",
    "title_dataset_val = title_dataset_val.map(to_new_format)\n",
    "title_dataset_test = title_dataset_test.map(to_new_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e513fc58-78b8-4aaf-a2b5-64fd44310616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Привет, дорогие мальчики и девочки! Ты знаешь, как быть здоровым ребенком? Вот некоторые правила, которым ты должен следовать.\n",
      "Во - первых, есть разные продукты, особенно фрукты и овощи. У вас может быть любимая еда, но вам лучше есть что-то другое, если вы едите разные продукты, вы, вероятно, получите больше питательных веществ, в которых нуждается ваше тело.\n",
      "Во-вторых, пить воду и молоко как можно чаще. Когда ты действительно хочешь пить, холодная вода - это \"Нет\". 1 выбор. Молоко - это отличный напиток, который может дать вам больше кальция, чтобы вырастить крепкие кости.\n",
      "В-третьих, слушай свое тело. Как ты себя чувствуешь, когда наелся? Когда вы едите, обращайте внимание на то, как чувствует себя ваше тело и когда ваш желудок чувствует себя комфортно насыщенным. Слишком много еды не сделает тебя комфортным и толстым.\n",
      "В-четвертых, ограничить время экрана. Время скриншота - это время, когда вы смотрите телевизор, DVD и видео или используете компьютеры. Приятно делать больше упражнений, таких как баскетбол, ездить на велосипеде и плавать. Ты не можешь смотреть телевизор больше двух часов в день.\n",
      "В-пятых, будьте активны. Одна вещь, которую ты хотел бы сделать в детстве, это выяснить, какую активность ты любишь больше всего. Найдите способы быть активными каждый день.\n",
      "Следуй этим правилам и можешь быть здоровым ребенком.\n",
      ".................................................................. ВОПРОС: Какое название лучше всего подойдёт для этого текста? ПРАВИЛЬНЫЙ ОТВЕТ: \"Как быть здоровым ребенком\".НЕПРАВИЛЬНЫЕ ВАРИАНТЫ ОТВЕТА: \n"
     ]
    }
   ],
   "source": [
    "print(title_dataset_test[220][\"inp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1bd755c6-cdc5-4d9a-a538-d093d89909ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Как быть активным\"; \"Как сделать себя важным\"; \"Как сделать твоих родителей здоровыми\"\n"
     ]
    }
   ],
   "source": [
    "print(title_dataset_test[220][\"distractors\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b55d10c3-044c-40ad-a8fd-76eae3e557cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a9a3db0c64a4e0395fa27d707559874",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4375 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99430378821e4c5082f30352215c493e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/219 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea5b1cf30d04464db61894304331c925",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/242 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "title_dataset_train = title_dataset_train.map(preprocess_function, batched=True, batch_size=2)\n",
    "title_dataset_val = title_dataset_val.map(preprocess_function, batched=True, batch_size=2)\n",
    "title_dataset_test = title_dataset_test.map(preprocess_function, batched=True, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1657b123-fcb7-4e95-8570-cba90001f990",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE  = 1\n",
    "NUM_TRAIN_EPOCHS = 20\n",
    "MODEL_NAME=\"RuT5-RACE-title-1\"\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=MODEL_NAME,\n",
    "    evaluation_strategy=\"epoch\", save_strategy=\"epoch\",\n",
    "    # evaluation_strategy=\"steps\", eval_steps=100, save_steps=100,\n",
    "    learning_rate=5e-5,\n",
    "    load_best_model_at_end=True,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "    prediction_loss_only=False,\n",
    "    gradient_checkpointing=True,\n",
    "    predict_with_generate=True, fp16=True,\n",
    "    eval_accumulation_steps=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4b4b4e3-3b58-4808-b94d-1a69a4153364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[49,\n",
       " 17944,\n",
       " 3803,\n",
       " 66,\n",
       " 8133,\n",
       " 38,\n",
       " 9218,\n",
       " 56,\n",
       " 13386,\n",
       " 49,\n",
       " 11290,\n",
       " 8571,\n",
       " 8308,\n",
       " 9,\n",
       " 9218,\n",
       " 13386,\n",
       " 49,\n",
       " 253,\n",
       " 2294,\n",
       " 1827,\n",
       " 425,\n",
       " 4,\n",
       " 83,\n",
       " 2]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_dataset_val[0][\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b35fde12-206c-4d02-9fd0-c52cddd18677",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metric_values(output: list[str], label_batch: list[str]) -> dict[str, Any]:\n",
    "    metric_dict = {\n",
    "        \"bleu\": bleu4.compute(predictions=output, references=[[label] for label in label_batch]),\n",
    "        \"sbleu\": sbleu.compute(predictions=output, references=[[label] for label in label_batch]),\n",
    "        \"rouge\": rouge.compute(predictions=output, references=label_batch),\n",
    "        \"meteor\": meteor.compute(predictions=output, references=label_batch)\n",
    "    }\n",
    "    return metric_dict\n",
    "\n",
    "def compute_metrics(eval_preds: EvalPrediction) -> dict[str, Any]:\n",
    "    preds = eval_preds.predictions\n",
    "    labels = eval_preds.label_ids\n",
    "\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = np.where(labels > 0, labels, tokenizer.pad_token_id)\n",
    "    labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    print(len(preds), len(labels))\n",
    "\n",
    "    preds = [\n",
    "        sent.replace(\"<pad>\", \" \").replace(\"</s>\", \" \").strip() for sent in preds\n",
    "    ]\n",
    "    labels = [\n",
    "        sent.replace(\"<pad>\", \" \").replace(\"</s>\", \" \").strip() for sent in labels\n",
    "    ]\n",
    "\n",
    "    metrics = compute_metric_values(preds, labels)\n",
    "    metric_dict = {\n",
    "        \"bleu\": metrics[\"bleu\"][\"bleu\"],\n",
    "        \"sbleu\": metrics[\"sbleu\"][\"score\"],\n",
    "        \"rouge1\": metrics[\"rouge\"][\"rouge1\"],\n",
    "        \"rouge2\": metrics[\"rouge\"][\"rouge2\"],\n",
    "        \"rougeL\": metrics[\"rouge\"][\"rougeL\"],\n",
    "        \"rougeLsum\": metrics[\"rouge\"][\"rougeLsum\"],\n",
    "        \"meteor\": metrics[\"meteor\"][\"meteor\"]\n",
    "    }\n",
    "    return metric_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "551f8e0c-aba1-41a8-86e7-76460caf16a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b46f1812-c690-4aa3-a491-64a98fb117af",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=title_dataset_train,\n",
    "    eval_dataset=title_dataset_val,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd92450a-57ca-41ac-9d99-c5c465aeb92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='87500' max='87500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [87500/87500 6:12:14, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Sbleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.404500</td>\n",
       "      <td>2.142511</td>\n",
       "      <td>0.144262</td>\n",
       "      <td>14.426197</td>\n",
       "      <td>0.006697</td>\n",
       "      <td>0.005327</td>\n",
       "      <td>0.006393</td>\n",
       "      <td>0.006697</td>\n",
       "      <td>0.353437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.064400</td>\n",
       "      <td>2.128983</td>\n",
       "      <td>0.148915</td>\n",
       "      <td>14.891484</td>\n",
       "      <td>0.006697</td>\n",
       "      <td>0.005327</td>\n",
       "      <td>0.006393</td>\n",
       "      <td>0.006697</td>\n",
       "      <td>0.349047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.792700</td>\n",
       "      <td>2.132879</td>\n",
       "      <td>0.141817</td>\n",
       "      <td>14.181709</td>\n",
       "      <td>0.007502</td>\n",
       "      <td>0.004871</td>\n",
       "      <td>0.008023</td>\n",
       "      <td>0.008219</td>\n",
       "      <td>0.331726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.492900</td>\n",
       "      <td>2.192235</td>\n",
       "      <td>0.149738</td>\n",
       "      <td>14.973796</td>\n",
       "      <td>0.008321</td>\n",
       "      <td>0.003044</td>\n",
       "      <td>0.008321</td>\n",
       "      <td>0.008219</td>\n",
       "      <td>0.335518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.265700</td>\n",
       "      <td>2.292400</td>\n",
       "      <td>0.156830</td>\n",
       "      <td>15.683014</td>\n",
       "      <td>0.005936</td>\n",
       "      <td>0.004566</td>\n",
       "      <td>0.005251</td>\n",
       "      <td>0.005936</td>\n",
       "      <td>0.343118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.117200</td>\n",
       "      <td>2.445708</td>\n",
       "      <td>0.159966</td>\n",
       "      <td>15.996632</td>\n",
       "      <td>0.006697</td>\n",
       "      <td>0.003044</td>\n",
       "      <td>0.006697</td>\n",
       "      <td>0.006697</td>\n",
       "      <td>0.346460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.932100</td>\n",
       "      <td>2.527000</td>\n",
       "      <td>0.160547</td>\n",
       "      <td>16.054672</td>\n",
       "      <td>0.010654</td>\n",
       "      <td>0.003044</td>\n",
       "      <td>0.010350</td>\n",
       "      <td>0.010350</td>\n",
       "      <td>0.338965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.805300</td>\n",
       "      <td>2.654886</td>\n",
       "      <td>0.162043</td>\n",
       "      <td>16.204321</td>\n",
       "      <td>0.008219</td>\n",
       "      <td>0.003044</td>\n",
       "      <td>0.008219</td>\n",
       "      <td>0.007306</td>\n",
       "      <td>0.336095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.698300</td>\n",
       "      <td>2.701871</td>\n",
       "      <td>0.165554</td>\n",
       "      <td>16.555427</td>\n",
       "      <td>0.007078</td>\n",
       "      <td>0.006088</td>\n",
       "      <td>0.006963</td>\n",
       "      <td>0.007078</td>\n",
       "      <td>0.337151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.560800</td>\n",
       "      <td>2.861351</td>\n",
       "      <td>0.169224</td>\n",
       "      <td>16.922440</td>\n",
       "      <td>0.012785</td>\n",
       "      <td>0.004566</td>\n",
       "      <td>0.011872</td>\n",
       "      <td>0.011416</td>\n",
       "      <td>0.342074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.516500</td>\n",
       "      <td>2.918833</td>\n",
       "      <td>0.177115</td>\n",
       "      <td>17.711525</td>\n",
       "      <td>0.003653</td>\n",
       "      <td>0.003044</td>\n",
       "      <td>0.003653</td>\n",
       "      <td>0.003653</td>\n",
       "      <td>0.345038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.411300</td>\n",
       "      <td>3.044558</td>\n",
       "      <td>0.183893</td>\n",
       "      <td>18.389299</td>\n",
       "      <td>0.012785</td>\n",
       "      <td>0.003044</td>\n",
       "      <td>0.012785</td>\n",
       "      <td>0.012785</td>\n",
       "      <td>0.357830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.359400</td>\n",
       "      <td>3.072307</td>\n",
       "      <td>0.178490</td>\n",
       "      <td>17.848977</td>\n",
       "      <td>0.007306</td>\n",
       "      <td>0.003044</td>\n",
       "      <td>0.007306</td>\n",
       "      <td>0.007306</td>\n",
       "      <td>0.347818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.293500</td>\n",
       "      <td>3.130601</td>\n",
       "      <td>0.179649</td>\n",
       "      <td>17.964919</td>\n",
       "      <td>0.014155</td>\n",
       "      <td>0.003044</td>\n",
       "      <td>0.014840</td>\n",
       "      <td>0.014155</td>\n",
       "      <td>0.339456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.250800</td>\n",
       "      <td>3.207313</td>\n",
       "      <td>0.187559</td>\n",
       "      <td>18.755856</td>\n",
       "      <td>0.009335</td>\n",
       "      <td>0.004349</td>\n",
       "      <td>0.009335</td>\n",
       "      <td>0.009335</td>\n",
       "      <td>0.350504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.207600</td>\n",
       "      <td>3.262269</td>\n",
       "      <td>0.186538</td>\n",
       "      <td>18.653779</td>\n",
       "      <td>0.011416</td>\n",
       "      <td>0.006088</td>\n",
       "      <td>0.010959</td>\n",
       "      <td>0.010731</td>\n",
       "      <td>0.352447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.185400</td>\n",
       "      <td>3.286116</td>\n",
       "      <td>0.194522</td>\n",
       "      <td>19.452207</td>\n",
       "      <td>0.011872</td>\n",
       "      <td>0.006697</td>\n",
       "      <td>0.011742</td>\n",
       "      <td>0.011481</td>\n",
       "      <td>0.363299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.161600</td>\n",
       "      <td>3.333371</td>\n",
       "      <td>0.197637</td>\n",
       "      <td>19.763735</td>\n",
       "      <td>0.014481</td>\n",
       "      <td>0.004871</td>\n",
       "      <td>0.014481</td>\n",
       "      <td>0.014351</td>\n",
       "      <td>0.359441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.140200</td>\n",
       "      <td>3.333735</td>\n",
       "      <td>0.195116</td>\n",
       "      <td>19.511614</td>\n",
       "      <td>0.009915</td>\n",
       "      <td>0.004871</td>\n",
       "      <td>0.009915</td>\n",
       "      <td>0.009785</td>\n",
       "      <td>0.355648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.119100</td>\n",
       "      <td>3.348525</td>\n",
       "      <td>0.196268</td>\n",
       "      <td>19.626809</td>\n",
       "      <td>0.009915</td>\n",
       "      <td>0.004871</td>\n",
       "      <td>0.009915</td>\n",
       "      <td>0.009785</td>\n",
       "      <td>0.360440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219 219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219 219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219 219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219 219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219 219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219 219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219 219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219 219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219 219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219 219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219 219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219 219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219 219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219 219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219 219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219 219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219 219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219 219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219 219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219 219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=87500, training_loss=0.7928069623238699, metrics={'train_runtime': 22335.4948, 'train_samples_per_second': 3.918, 'train_steps_per_second': 3.918, 'total_flos': 4.42172887805952e+16, 'train_loss': 0.7928069623238699, 'epoch': 20.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
